\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[]{neurips_2019}
\usepackage{arxiv}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hidelinks]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}


\newcommand{\etal}{\textit{et al}.}
\newcommand{\ie}{\textit{i}.\textit{e}.}
\newcommand{\eg}{\textit{e}.\textit{g}.}

\newcommand{\MW}[1]{{\color{blue} {\bf (MW: #1)}}}
\newcommand{\AY}[1]{{\color{magenta} {\bf (AY: #1)}}}

\newcommand{\newl}{\newline}
\newcommand{\bm}{\mathbf}

\newcommand{\bW}{\bm{W}}
\newcommand{\bw}{\bm{w}}
\newcommand{\bV}{\bm{V}}
\newcommand{\bD}{\bm{D}}
\newcommand{\bx}{\bm{x}}
\newcommand{\bX}{\bm{X}}
\newcommand{\bh}{\bm{h}}
\newcommand{\bz}{\bm{z}}
\newcommand{\by}{\bm{y}}
\newcommand{\bY}{\bm{Y}}
\newcommand{\bb}{\bm{b}}
\newcommand{\bs}{\bm{s}}

\usepackage[format=plain,
            font=small,
            labelfont=it,
            textfont=it]{caption}


\title{Frequency dilation learning for temporal convolutional neural networks}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{Moritz Wolter\\
 Institute for Computer Science\\
 University of Bonn\\
 {\tt\small wolter@cs.uni-bonn.de}
 \And
 Angela Yao\\
 School of Computing\\
 National University of Singapore\\
 {\tt\small ayao@comp.nus.edu.sg}
}

\begin{document}

\maketitle

\begin{abstract}
Convolutional neural networks (CNNs) make up the bedrock in modern machine learning. With ever increasing data sets and increasing efforts to port these foundational systems to mobile and robotics applications there is an ever strong desire to reduce the computational and memory footprint of CNNs. In this paper we introduce frequency dilation, a novel technique to increase efficiency at minimal cost in terms of network accuracy.
\end{abstract}

\section{Introduction}
Integral transforms lie at the core of computational efficiency trough sparsity \cite{Mallat}\cite{Strang1997}. Previous works have reduced the parameter count of fully connected layers trough the fast Hadamard transforms \cite{yang2015deep}. Fully connected layers are re-parametrized trough a fixed basis. Using a fixed bases \cite{Arjovsky2016} proposes a unitary RNN, later \cite{Wisdom2016} finds that in the RNN case using a fixed basis is detrimental to network performance. 
As we believe that the fixed basis in \cite{yang2015deep} is equally restricted, and therefore detrimental to performance. Instead we proposed a learn-able representation based on the dual tree wavelet transform \cite{Selesnick2005}, which we apply to both input data and network weights. 

\section{Related work}
Learnable filters \cite{Recoskie2018}

Dimensionality reduction in inputs (FourierRNN)

Dimensionality reduction in weights \cite{yang2015deep}

\section{Theory}
Sparsity of a representation is basis dependent~\cite{Strang1997}\cite{Strang1994}, fewer coefficients are are required in order to represent a signal, only a limited number of basis functions components are able to do the job.

\subsection{Single tree filter design}
Wavelet filter design 

Perfect reconstruction (PR) \cite{Selesnick2005}\cite[page 107]{Strang1997} requires, no distortion:
\begin{equation}
H_0(z) F_{0}(z) + H_1(z) F_1(z) = 2
\end{equation}
and alias cancellation:
\begin{equation}
H_0(-z) F_{0}(z) + H_1(-z) F_1(z) = 0
\end{equation}
For alias cancellation $F_0(z) = H_1(-z)$, $F_1(z) = -H_0(-z)$ is typically chosen \cite{Strang1994}. A product filter approach is chosen to deal with the reconstruction condition.
\begin{equation}
P_0(z) = F_0(z)H_0(z); P_1(z) = F_1(z)H_1(z) 
\end{equation}
Alias cancellation leads to $P_1(z) = -P_0(-z)$ and turns the no distortion condition into:
\begin{equation}
P(z) + P(-z) = 2
\end{equation}
Even powers have to cancel, odd powers are design variables.

\subsection{Dual vs. single Tree approach}

\subsection{The dual tree wavelet approach}

Dual tree wavelet filters should satisfy \cite{Selesnick2005}:
\begin{itemize}
\item approximate half sample delay property (?).
\item Perfect reconstruction (orthogonal or bi-orthogonal)
\item finite support (FIR filters)
\item vanishing moments good stopband
\item linear phase filters (desired, but not required)
\end{itemize}

Where and how does the phase shift condition fit into this?

The wavelet literature tells us $\int_{-\infty}^{\infty} \Psi(t) dt = 0$
and $\int_{-\infty}^{\infty} \| \Psi(t) \| dt = 1$.

\subsection{Biorthogonal filters}




\subsubsection{Learning biorthogonal filters}
Which constraints are required?

\subsection{Multi-resolution analysis}
Top-down vs bottom up in image processing, relations?


\section{Experiments}
Compare, compression using Fourier, Harr, and learned wavelets on mackey glass and a bumpy
rectangularly time series for example a staircase mackey glass or lorenz.

\newpage
{\small
\bibliographystyle{plain}
\bibliography{bib}
}

\end{document}