\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[]{neurips_2019}
\usepackage{arxiv}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hidelinks]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}


\newcommand{\etal}{\textit{et al}.}
\newcommand{\ie}{\textit{i}.\textit{e}.}
\newcommand{\eg}{\textit{e}.\textit{g}.}

\newcommand{\MW}[1]{{\color{blue} {\bf (MW: #1)}}}
\newcommand{\AY}[1]{{\color{magenta} {\bf (AY: #1)}}}

\newcommand{\newl}{\newline}
\newcommand{\bm}{\mathbf}

\newcommand{\bW}{\bm{W}}
\newcommand{\bw}{\bm{w}}
\newcommand{\bV}{\bm{V}}
\newcommand{\bD}{\bm{D}}
\newcommand{\bx}{\bm{x}}
\newcommand{\bX}{\bm{X}}
\newcommand{\bh}{\bm{h}}
\newcommand{\bz}{\bm{z}}
\newcommand{\by}{\bm{y}}
\newcommand{\bY}{\bm{Y}}
\newcommand{\bb}{\bm{b}}
\newcommand{\bs}{\bm{s}}

\usepackage[format=plain,
            font=small,
            labelfont=it,
            textfont=it]{caption}


\title{Frequency dilation learning for temporal convolutional neural networks}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

% \author{Moritz Wolter\\
%  Institute for Computer Science\\
%  University of Bonn\\
%  {\tt\small wolter@cs.uni-bonn.de}
%  \And
%  Angela Yao\\
%  School of Computing\\
%  National University of Singapore\\
%  {\tt\small ayao@comp.nus.edu.sg}
% }

\begin{document}

\maketitle

\begin{abstract}
Convolutional neural networks (CNNs) make up the bedrock in modern machine learning. With ever increasing data sets and increasing efforts to port these foundational systems to mobile and robotics applications there is an ever strong desire to reduce the computational and memory footprint of CNNs. In this paper we introduce frequency dilation, a novel technique to increase efficiency at minimal cost in terms of network accuracy.
\end{abstract}

\section{Introduction}
Integral transforms lie at the core of computational efficiency trough sparsity \cite{Mallat}\cite{Strang1997}. Previous works have reduced the parameter count of fully connected layers trough the fast Hadamard transforms \cite{yang2015deep}. Fully connected layers are re-parametrized trough a fixed basis. Using a fixed bases \cite{Arjovsky2016} proposes a unitary RNN, later \cite{Wisdom2016} finds that in the RNN case using a fixed basis is detrimental to network performance. 
As we believe that the fixed basis in \cite{yang2015deep} is equally restricted, and therefore detrimental to performance. Instead we proposed a learn-able representation based on the dual tree wavelet transform \cite{Selesnick2005}, which we apply to both input data and network weights. 

\section{Related work}
Learnable filters \cite{Recoskie2018}

Dimensionality reduction in inputs (FourierRNN)

Dimensionality reduction in weights \cite{yang2015deep}

\section{Theory}
Dilation in modern convolutional neural networks is equivalent to low-pass filtering in the frequency domain. This connection becomes apparent if one looks at the dilated CNN output as a downsampled version of the original CNN. Down-sampling is a form of low-pass filtering, because high frequency information present above frequencies not covered by the new Nyquist rate vanishes.
When using dilated CNNs one broadens the receptive field by ignoring high frequency information. But depending on the data this may not be the best approach. In this work we explore an alternative dilation method. Instead of hard coding a low pass filter we explore broadening the receptive field trough compressive wavelet basis learning. 
The data is represented in terms of a learned wavelet basis and only a small number of coefficients is kept. In order to minmize the loss the optimizer will be forced to move the wavelet-filters into frequency bands where most of the relevant information is present. 
Sparsity and thereby the effectiveness of a representation is basis dependent~\cite{Strang1997}\cite{Strang1994}, when a suitable basis is chosen few coefficients are are required in order to represent a signal, only a limited number of basis functions components are able to do the job.



\subsection{Single tree wavelet basis-optimization}
Wavelets and filter banks are closely related. This section explores how methods from the field of digital filter design can be used to optimize wavlet basis representations.
\subsubsection{Filter coefficient optimization}
Perfect reconstruction (PR) \cite{Selesnick2005}\cite[page 107]{Strang1997} requires, no distortion:
\begin{equation}
H_0(z) F_{0}(z) + H_1(z) F_1(z) = 2
\end{equation}
and alias cancellation:
\begin{equation}
H_0(-z) F_{0}(z) + H_1(-z) F_1(z) = 0
\end{equation}
For alias cancellation $F_0(z) = H_1(-z)$, $F_1(z) = -H_0(-z)$ is typically chosen \cite{Strang1994}. A product filter approach is chosen to deal with the reconstruction condition.
\begin{equation}
P_0(z) = F_0(z)H_0(z); P_1(z) = F_1(z)H_1(z) 
\end{equation}
Alias cancellation leads to $P_1(z) = -P_0(-z)$ and turns the no distortion condition into:
\begin{equation}
P(z) + P(-z) = 2
\end{equation}
Even powers have to be zero, odd powers are design variables\cite[page 107]{Strang1997}. In theory it should be possible to optimize the odd power coefficients by gradient descent. 
One way designers choose the coeffecients is to picke the coefficients for the zeroth phase and generate subsequent phase filters by alternating the signs or flip-inversion~\cite[page 109]{Strang1997}. After hard coding the alternation or inversion conditions, this process should lend itself to optimization by gradient descent.

\subsubsection{Orthogonal polyphase matrix optimization}
The filter design problem can be expressed in terms of choosing the analysis and reconstruction polyphase matrices. A polyphase system requires $\mathbf{F}_p \mathbf{H}_p = \mathbf{I}$, to work~\cite[page 116]{Strang1997}. In the orthogonal case we have $\mathbf{H}_p = \mathbf{F}_p^{-1}$. Given an initial orthogonal wavelet basis it should be possible to use Stiefel manifold optimization as outlined in \cite{Wisdom2016}\cite{wolter-2018-neurips}, to solve the wavelet optimization problem by using:
\begin{align}
\mathbf{H}_{p, k+1} = (\mathbf{I} + \frac{\lambda}{2}\mathbf{A}_k)^{-1}(\mathbf{I} - \frac{\lambda}{2}\mathbf{A}_k)\mathbf{H}_{p, k}
\end{align}
Where $\mathbf{A}_k = \mathbf{H}_{p,k}\overline{\bigtriangledown_{w}F}^T - \overline{\mathbf{H}_{p, k}}^T\bigtriangledown_{w}F$, with the cost function F and $\mathbf{H}_{p,k}$ the orthogonal polyphase matrix at time k. Ideally the pros and cons of orthogonal wavelets will become apparent in comparison to the biorthogonal case described before.


\subsection{Dual vs. single Tree approach}
\MW{TODO: Write}

\subsection{The dual tree wavelet basis optimization}
\MW{I think dual tree wavelet basis optimization is based on using single tree methods plus the phase constraint, which connects the two trees. Dual tree wavelets are always complex valued, so I am hoping to connect this to \cite{wolter-2018-neurips}. I am going to finish this section later: \\
Dual tree wavelet filters should satisfy \cite{Selesnick2005}:
\begin{itemize}
\item approximate half sample delay property (?).
\item Perfect reconstruction (orthogonal or bi-orthogonal)
\item finite support (FIR filters)
\item vanishing moments good stopband
\item linear phase filters (desired, but not required)
\end{itemize}

Where and how does the phase shift condition fit into this?

The wavelet literature [mallat] tells us $\int_{-\infty}^{\infty} \Psi(t) dt = 0$
and $\int_{-\infty}^{\infty} \| \Psi(t) \| dt = 1$.}


\subsection{Multi-resolution analysis}
Top-down vs bottom up in image processing, relations?


\section{Experiments}
\subsection{Time series compression, is the wavelet basis code ok?}
Compare a highly compressed example using Fourier, Harr, and learned wavelets on mackey glass and a bumpy rectangularly time series for example a staircase mackey glass or lorenz. \\

\MW{TODO: Choose a good real data source. To test this too.}

\subsection{CNN compression, or can we use wavelets to replace the Welsh-Hadamard transform?}
Revisit \cite{yang2015deep} deploy an optimized basis and see if we can do better.


\subsection{Frequency dilation, or can wavelets help us broaden CNN receptive fields?}
I am not yet sure how to best explore this idea.

{\small
\bibliographystyle{plain}
\bibliography{bib}
}

\end{document}