Namespace(batch_size=50, cell='WaveletGRU', compression_mode='full', hidden=512, lr=0.001, n_test=10000, n_train=600000, problem='memory', time_steps=150)
Baseline is 0.12232009068704916
full compression
Creating a Wavelet GRU, do not forget to add the wavelet-loss.
torch.Size([512])
torch.Size([545])
torch.Size([512])
torch.Size([6])
torch.Size([6])
torch.Size([6])
torch.Size([6])
torch.Size([512, 10])
torch.Size([512])
torch.Size([512])
torch.Size([545])
torch.Size([512])
torch.Size([6])
torch.Size([6])
torch.Size([6])
torch.Size([6])
torch.Size([512, 10])
torch.Size([512])
torch.Size([512])
torch.Size([545])
torch.Size([512])
torch.Size([6])
torch.Size([6])
torch.Size([6])
torch.Size([6])
torch.Size([512, 10])
torch.Size([512])
torch.Size([10, 512])
torch.Size([10])
parameter total 26805
step 0 loss 2.1350107 baseline: 0.12232009068704916 acc 0.074 wl 1.6338419e-12 train True
step 50 loss 0.3360569 baseline: 0.12232009068704916 acc 0.0 wl 0.00011445679 train True
step 100 loss 0.3403708 baseline: 0.12232009068704916 acc 0.0 wl 0.0001556587 train True
step 150 loss 0.30840868 baseline: 0.12232009068704916 acc 0.0 wl 0.00018693136 train True
step 200 loss 0.31696543 baseline: 0.12232009068704916 acc 0.0 wl 0.00017206508 train True
step 250 loss 0.2919963 baseline: 0.12232009068704916 acc 0.0 wl 0.00014817165 train True
step 300 loss 0.2974911 baseline: 0.12232009068704916 acc 0.0 wl 0.00028612878 train True
step 350 loss 0.253334 baseline: 0.12232009068704916 acc 0.0 wl 0.00031230267 train True
step 400 loss 0.25905898 baseline: 0.12232009068704916 acc 0.0 wl 0.00027260903 train True
step 450 loss 0.23858601 baseline: 0.12232009068704916 acc 0.018 wl 0.0002669988 train True
step 500 loss 0.20961411 baseline: 0.12232009068704916 acc 0.0 wl 0.00026867836 train True
step 550 loss 0.18380639 baseline: 0.12232009068704916 acc 0.058 wl 0.00048689364 train True
step 600 loss 0.16534007 baseline: 0.12232009068704916 acc 0.07 wl 0.0006525213 train True
step 650 loss 0.14815114 baseline: 0.12232009068704916 acc 0.056 wl 0.00069808366 train True
step 700 loss 0.14198689 baseline: 0.12232009068704916 acc 0.074 wl 0.0011439787 train True
step 750 loss 0.13278747 baseline: 0.12232009068704916 acc 0.09 wl 0.0011566925 train True
step 800 loss 0.13812427 baseline: 0.12232009068704916 acc 0.076 wl 0.0010389796 train True
step 850 loss 0.12588038 baseline: 0.12232009068704916 acc 0.132 wl 0.00087672344 train True
step 900 loss 0.12534283 baseline: 0.12232009068704916 acc 0.12 wl 0.00065571076 train True
step 950 loss 0.12713222 baseline: 0.12232009068704916 acc 0.084 wl 0.00047654586 train True
step 1000 loss 0.13893704 baseline: 0.12232009068704916 acc 0.122 wl 0.0055899397 train True
step 1050 loss 0.12505963 baseline: 0.12232009068704916 acc 0.138 wl 0.00024609242 train True
step 1100 loss 0.1253086 baseline: 0.12232009068704916 acc 0.14 wl 0.0006033137 train True
step 1150 loss 0.12806346 baseline: 0.12232009068704916 acc 0.104 wl 0.00025428244 train True
step 1200 loss 0.123745635 baseline: 0.12232009068704916 acc 0.124 wl 0.00013099126 train True
step 1250 loss 0.12315919 baseline: 0.12232009068704916 acc 0.112 wl 0.00013964971 train True
step 1300 loss 0.123619586 baseline: 0.12232009068704916 acc 0.104 wl 0.00018878898 train True
step 1350 loss 0.12284649 baseline: 0.12232009068704916 acc 0.122 wl 0.00020552718 train True
step 1400 loss 0.1227225 baseline: 0.12232009068704916 acc 0.116 wl 0.00011972657 train True
step 1450 loss 0.12281961 baseline: 0.12232009068704916 acc 0.118 wl 8.847135e-05 train True
step 1500 loss 0.12268049 baseline: 0.12232009068704916 acc 0.118 wl 7.216194e-05 train True
step 1550 loss 0.12248076 baseline: 0.12232009068704916 acc 0.14 wl 0.00025112223 train True
step 1600 loss 0.12328201 baseline: 0.12232009068704916 acc 0.126 wl 9.630273e-05 train True
step 1650 loss 0.1234566 baseline: 0.12232009068704916 acc 0.104 wl 7.155466e-05 train True
step 1700 loss 0.12257547 baseline: 0.12232009068704916 acc 0.126 wl 7.814249e-05 train True
step 1750 loss 0.12271996 baseline: 0.12232009068704916 acc 0.09 wl 0.00017585866 train True
step 1800 loss 0.122344166 baseline: 0.12232009068704916 acc 0.126 wl 0.00019676606 train True
step 1850 loss 0.1224407 baseline: 0.12232009068704916 acc 0.118 wl 0.0001786509 train True
step 1900 loss 0.1224511 baseline: 0.12232009068704916 acc 0.14 wl 6.8517984e-05 train True
step 1950 loss 0.12307063 baseline: 0.12232009068704916 acc 0.12 wl 7.0721195e-05 train True
step 2000 loss 0.12251186 baseline: 0.12232009068704916 acc 0.114 wl 7.413405e-05 train True
step 2050 loss 0.12256327 baseline: 0.12232009068704916 acc 0.138 wl 0.00015066328 train True
step 2100 loss 0.1223507 baseline: 0.12232009068704916 acc 0.122 wl 0.00022161855 train True
step 2150 loss 0.122184224 baseline: 0.12232009068704916 acc 0.14 wl 0.00017086275 train True
step 2200 loss 0.12229115 baseline: 0.12232009068704916 acc 0.14 wl 0.00016532143 train True
step 2250 loss 0.12253721 baseline: 0.12232009068704916 acc 0.122 wl 0.00011576949 train True
step 2300 loss 0.12282568 baseline: 0.12232009068704916 acc 0.114 wl 0.000113402624 train True
step 2350 loss 0.12264888 baseline: 0.12232009068704916 acc 0.108 wl 0.00012412225 train True
step 2400 loss 0.1222308 baseline: 0.12232009068704916 acc 0.152 wl 0.0001231048 train True
step 2450 loss 0.12270463 baseline: 0.12232009068704916 acc 0.116 wl 0.00014986852 train True
step 2500 loss 0.12228158 baseline: 0.12232009068704916 acc 0.134 wl 0.00011193678 train True
step 2550 loss 0.12225741 baseline: 0.12232009068704916 acc 0.136 wl 0.00010173237 train True
step 2600 loss 0.12244758 baseline: 0.12232009068704916 acc 0.136 wl 0.00011310538 train True
step 2650 loss 0.12262048 baseline: 0.12232009068704916 acc 0.108 wl 0.00022443093 train True
step 2700 loss 0.122360066 baseline: 0.12232009068704916 acc 0.134 wl 0.00010939142 train True
step 2750 loss 0.12205519 baseline: 0.12232009068704916 acc 0.142 wl 0.00013156721 train True
step 2800 loss 0.12250934 baseline: 0.12232009068704916 acc 0.12 wl 0.00010870074 train True
step 2850 loss 0.122504555 baseline: 0.12232009068704916 acc 0.11 wl 0.00011283695 train True
step 2900 loss 0.122824565 baseline: 0.12232009068704916 acc 0.124 wl 0.00012479501 train True
step 2950 loss 0.12265911 baseline: 0.12232009068704916 acc 0.12 wl 0.000159149 train True
step 3000 loss 0.1223513 baseline: 0.12232009068704916 acc 0.128 wl 0.000114904484 train True
step 3050 loss 0.12235439 baseline: 0.12232009068704916 acc 0.116 wl 0.00017329628 train True
step 3100 loss 0.12234684 baseline: 0.12232009068704916 acc 0.136 wl 9.280443e-05 train True
step 3150 loss 0.122461796 baseline: 0.12232009068704916 acc 0.132 wl 0.00010344402 train True
step 3200 loss 0.12277443 baseline: 0.12232009068704916 acc 0.09 wl 0.00012632593 train True
step 3250 loss 0.12239196 baseline: 0.12232009068704916 acc 0.138 wl 0.00014449848 train True
step 3300 loss 0.122784264 baseline: 0.12232009068704916 acc 0.118 wl 0.00014875403 train True
step 3350 loss 0.12238004 baseline: 0.12232009068704916 acc 0.114 wl 0.00012409393 train True
step 3400 loss 0.12248234 baseline: 0.12232009068704916 acc 0.142 wl 0.00012215911 train True
step 3450 loss 0.12254481 baseline: 0.12232009068704916 acc 0.136 wl 0.00016461173 train True
step 3500 loss 0.12241969 baseline: 0.12232009068704916 acc 0.118 wl 0.00015486246 train True
step 3550 loss 0.12215685 baseline: 0.12232009068704916 acc 0.13 wl 0.00012996954 train True
step 3600 loss 0.122548714 baseline: 0.12232009068704916 acc 0.126 wl 0.000126746 train True
step 3650 loss 0.12208482 baseline: 0.12232009068704916 acc 0.134 wl 0.00011492001 train True
step 3700 loss 0.12238122 baseline: 0.12232009068704916 acc 0.136 wl 0.00013433216 train True
step 3750 loss 0.12229945 baseline: 0.12232009068704916 acc 0.144 wl 0.00013852768 train True
step 3800 loss 0.122463666 baseline: 0.12232009068704916 acc 0.118 wl 0.00012250672 train True
step 3850 loss 0.12239058 baseline: 0.12232009068704916 acc 0.144 wl 0.00017201394 train True
step 3900 loss 0.12235025 baseline: 0.12232009068704916 acc 0.146 wl 0.00018228962 train True
step 3950 loss 0.12234234 baseline: 0.12232009068704916 acc 0.128 wl 0.00014823649 train True
step 4000 loss 0.12257693 baseline: 0.12232009068704916 acc 0.14 wl 0.0001646899 train True
step 4050 loss 0.122513324 baseline: 0.12232009068704916 acc 0.132 wl 0.00017278371 train True
step 4100 loss 0.12230922 baseline: 0.12232009068704916 acc 0.142 wl 0.00017143525 train True
step 4150 loss 0.12248897 baseline: 0.12232009068704916 acc 0.124 wl 0.00015330494 train True
step 4200 loss 0.122283235 baseline: 0.12232009068704916 acc 0.136 wl 0.00014647655 train True
step 4250 loss 0.12221923 baseline: 0.12232009068704916 acc 0.118 wl 0.00015678411 train True
step 4300 loss 0.12242483 baseline: 0.12232009068704916 acc 0.122 wl 0.000194601 train True
step 4350 loss 0.12250141 baseline: 0.12232009068704916 acc 0.098 wl 0.00017018887 train True
step 4400 loss 0.12219517 baseline: 0.12232009068704916 acc 0.128 wl 0.00022657626 train True
step 4450 loss 0.12266136 baseline: 0.12232009068704916 acc 0.114 wl 0.000279873 train True
step 4500 loss 0.122558564 baseline: 0.12232009068704916 acc 0.126 wl 0.00020261848 train True
step 4550 loss 0.12245845 baseline: 0.12232009068704916 acc 0.128 wl 0.0001909809 train True
step 4600 loss 0.122403264 baseline: 0.12232009068704916 acc 0.12 wl 0.0001482435 train True
step 4650 loss 0.12255832 baseline: 0.12232009068704916 acc 0.108 wl 0.00012477253 train True
step 4700 loss 0.12248923 baseline: 0.12232009068704916 acc 0.108 wl 0.00011472936 train True
step 4750 loss 0.12261433 baseline: 0.12232009068704916 acc 0.102 wl 0.00013155909 train True
step 4800 loss 0.12234424 baseline: 0.12232009068704916 acc 0.112 wl 0.00014453198 train True
step 4850 loss 0.122584604 baseline: 0.12232009068704916 acc 0.098 wl 0.00017218513 train True
step 4900 loss 0.12218802 baseline: 0.12232009068704916 acc 0.136 wl 0.00018060337 train True
step 4950 loss 0.12273343 baseline: 0.12232009068704916 acc 0.122 wl 0.00014512539 train True
step 5000 loss 0.12236937 baseline: 0.12232009068704916 acc 0.13 wl 0.00014254838 train True
step 5050 loss 0.12227783 baseline: 0.12232009068704916 acc 0.106 wl 0.00020756942 train True
step 5100 loss 0.12238798 baseline: 0.12232009068704916 acc 0.13 wl 0.00018400731 train True
step 5150 loss 0.12265605 baseline: 0.12232009068704916 acc 0.114 wl 0.00020098704 train True
step 5200 loss 0.122744285 baseline: 0.12232009068704916 acc 0.106 wl 0.00016667672 train True
step 5250 loss 0.12242222 baseline: 0.12232009068704916 acc 0.114 wl 0.00014392371 train True
step 5300 loss 0.12242799 baseline: 0.12232009068704916 acc 0.13 wl 0.00013585173 train True
step 5350 loss 0.122347645 baseline: 0.12232009068704916 acc 0.122 wl 0.00012384492 train True
step 5400 loss 0.12224365 baseline: 0.12232009068704916 acc 0.12 wl 0.000103338505 train True
step 5450 loss 0.122607075 baseline: 0.12232009068704916 acc 0.116 wl 0.0001020277 train True
step 5500 loss 0.12222847 baseline: 0.12232009068704916 acc 0.15 wl 0.00015681796 train True
step 5550 loss 0.12247833 baseline: 0.12232009068704916 acc 0.106 wl 0.00014737264 train True
step 5600 loss 0.12229973 baseline: 0.12232009068704916 acc 0.106 wl 0.0001480644 train True
step 5650 loss 0.12234849 baseline: 0.12232009068704916 acc 0.136 wl 0.00013342066 train True
step 5700 loss 0.12233359 baseline: 0.12232009068704916 acc 0.13 wl 0.000134881 train True
step 5750 loss 0.12221606 baseline: 0.12232009068704916 acc 0.138 wl 0.00011767251 train True
step 5800 loss 0.12226403 baseline: 0.12232009068704916 acc 0.13 wl 0.00013416247 train True
step 5850 loss 0.12236228 baseline: 0.12232009068704916 acc 0.126 wl 0.00013640957 train True
step 5900 loss 0.1223524 baseline: 0.12232009068704916 acc 0.128 wl 0.00010821046 train True
step 5950 loss 0.12236552 baseline: 0.12232009068704916 acc 0.102 wl 0.00012632432 train True
step 6000 loss 0.12243397 baseline: 0.12232009068704916 acc 0.124 wl 0.00013794855 train True
step 6050 loss 0.12250674 baseline: 0.12232009068704916 acc 0.14 wl 0.00014625618 train True
step 6100 loss 0.1224924 baseline: 0.12232009068704916 acc 0.13 wl 0.000119926815 train True
step 6150 loss 0.1226219 baseline: 0.12232009068704916 acc 0.092 wl 0.0001328777 train True
step 6200 loss 0.12235502 baseline: 0.12232009068704916 acc 0.156 wl 0.0001173832 train True
step 6250 loss 0.12225608 baseline: 0.12232009068704916 acc 0.148 wl 0.00011620358 train True
step 6300 loss 0.12236739 baseline: 0.12232009068704916 acc 0.12 wl 0.00017330605 train True
step 6350 loss 0.122335464 baseline: 0.12232009068704916 acc 0.112 wl 0.00013153774 train True
step 6400 loss 0.12259033 baseline: 0.12232009068704916 acc 0.134 wl 0.00014574878 train True
step 6450 loss 0.122370236 baseline: 0.12232009068704916 acc 0.122 wl 0.00012595706 train True
step 6500 loss 0.12231741 baseline: 0.12232009068704916 acc 0.134 wl 0.000113345195 train True
step 6550 loss 0.12237227 baseline: 0.12232009068704916 acc 0.138 wl 0.00011525766 train True
step 6600 loss 0.12218153 baseline: 0.12232009068704916 acc 0.136 wl 0.00012826691 train True
step 6650 loss 0.12249731 baseline: 0.12232009068704916 acc 0.106 wl 0.0001676571 train True
step 6700 loss 0.12248084 baseline: 0.12232009068704916 acc 0.12 wl 0.00017121622 train True
step 6750 loss 0.122344844 baseline: 0.12232009068704916 acc 0.124 wl 0.00017388267 train True
step 6800 loss 0.12230027 baseline: 0.12232009068704916 acc 0.134 wl 0.00014532692 train True
step 6850 loss 0.12239627 baseline: 0.12232009068704916 acc 0.118 wl 0.00022279109 train True
step 6900 loss 0.12237843 baseline: 0.12232009068704916 acc 0.112 wl 0.00010891985 train True
step 6950 loss 0.12242239 baseline: 0.12232009068704916 acc 0.114 wl 0.000144263 train True
step 7000 loss 0.122255616 baseline: 0.12232009068704916 acc 0.12 wl 0.00012643826 train True
step 7050 loss 0.12246295 baseline: 0.12232009068704916 acc 0.12 wl 0.00014355377 train True
step 7100 loss 0.12239483 baseline: 0.12232009068704916 acc 0.146 wl 0.00014053096 train True
step 7150 loss 0.122291304 baseline: 0.12232009068704916 acc 0.134 wl 0.00012177691 train True
step 7200 loss 0.12235906 baseline: 0.12232009068704916 acc 0.134 wl 0.0001307651 train True
step 7250 loss 0.122453324 baseline: 0.12232009068704916 acc 0.122 wl 0.00013490772 train True
step 7300 loss 0.12229166 baseline: 0.12232009068704916 acc 0.136 wl 0.00014036262 train True
step 7350 loss 0.122435644 baseline: 0.12232009068704916 acc 0.154 wl 0.00012966101 train True
step 7400 loss 0.12266866 baseline: 0.12232009068704916 acc 0.094 wl 0.00012353781 train True
step 7450 loss 0.12237566 baseline: 0.12232009068704916 acc 0.128 wl 0.00015881102 train True
step 7500 loss 0.12235517 baseline: 0.12232009068704916 acc 0.086 wl 0.00017529276 train True
step 7550 loss 0.122307055 baseline: 0.12232009068704916 acc 0.136 wl 0.00016220815 train True
step 7600 loss 0.12244501 baseline: 0.12232009068704916 acc 0.116 wl 0.00014106289 train True
step 7650 loss 0.12240286 baseline: 0.12232009068704916 acc 0.102 wl 0.00017515669 train True
step 7700 loss 0.12240816 baseline: 0.12232009068704916 acc 0.118 wl 0.000112141715 train True
step 7750 loss 0.122184284 baseline: 0.12232009068704916 acc 0.144 wl 0.00010319818 train True
step 7800 loss 0.12233358 baseline: 0.12232009068704916 acc 0.136 wl 0.00012309778 train True
step 7850 loss 0.12206271 baseline: 0.12232009068704916 acc 0.146 wl 0.00017963263 train True
step 7900 loss 0.12235438 baseline: 0.12232009068704916 acc 0.134 wl 0.0001447792 train True
step 7950 loss 0.12241844 baseline: 0.12232009068704916 acc 0.124 wl 0.00015488343 train True
step 8000 loss 0.122260064 baseline: 0.12232009068704916 acc 0.13 wl 0.00016284635 train True
step 8050 loss 0.122736484 baseline: 0.12232009068704916 acc 0.11 wl 0.00015621229 train True
step 8100 loss 0.12236717 baseline: 0.12232009068704916 acc 0.128 wl 0.00013562807 train True
step 8150 loss 0.1222962 baseline: 0.12232009068704916 acc 0.12 wl 0.00014393002 train True
step 8200 loss 0.12229369 baseline: 0.12232009068704916 acc 0.122 wl 0.0001492463 train True
step 8250 loss 0.12240297 baseline: 0.12232009068704916 acc 0.106 wl 0.00013300369 train True
step 8300 loss 0.122329734 baseline: 0.12232009068704916 acc 0.098 wl 0.00011355044 train True
step 8350 loss 0.12248249 baseline: 0.12232009068704916 acc 0.106 wl 0.00011860467 train True
step 8400 loss 0.12234556 baseline: 0.12232009068704916 acc 0.144 wl 0.00013029312 train True
step 8450 loss 0.12234943 baseline: 0.12232009068704916 acc 0.1 wl 0.00013600453 train True
step 8500 loss 0.122276165 baseline: 0.12232009068704916 acc 0.126 wl 0.00011842391 train True
step 8550 loss 0.12241373 baseline: 0.12232009068704916 acc 0.126 wl 0.000114917624 train True
step 8600 loss 0.12232644 baseline: 0.12232009068704916 acc 0.114 wl 0.00013554173 train True
step 8650 loss 0.12240401 baseline: 0.12232009068704916 acc 0.158 wl 9.877517e-05 train True
step 8700 loss 0.12250995 baseline: 0.12232009068704916 acc 0.114 wl 0.00010249516 train True
step 8750 loss 0.122183174 baseline: 0.12232009068704916 acc 0.158 wl 0.0001230376 train True
step 8800 loss 0.1222933 baseline: 0.12232009068704916 acc 0.144 wl 0.00017300338 train True
step 8850 loss 0.12242061 baseline: 0.12232009068704916 acc 0.122 wl 0.00015283594 train True
step 8900 loss 0.12224837 baseline: 0.12232009068704916 acc 0.13 wl 0.00015938655 train True
step 8950 loss 0.12229294 baseline: 0.12232009068704916 acc 0.128 wl 0.00015876735 train True
step 9000 loss 0.12241819 baseline: 0.12232009068704916 acc 0.136 wl 0.00014173979 train True
step 9050 loss 0.12236042 baseline: 0.12232009068704916 acc 0.136 wl 0.00013825388 train True
step 9100 loss 0.12213522 baseline: 0.12232009068704916 acc 0.156 wl 0.0001170839 train True
step 9150 loss 0.1222096 baseline: 0.12232009068704916 acc 0.134 wl 0.00012611599 train True
step 9200 loss 0.12214274 baseline: 0.12232009068704916 acc 0.148 wl 0.00011782948 train True
step 9250 loss 0.12222724 baseline: 0.12232009068704916 acc 0.134 wl 0.00012154532 train True
step 9300 loss 0.12241373 baseline: 0.12232009068704916 acc 0.134 wl 0.00014475748 train True
step 9350 loss 0.122381695 baseline: 0.12232009068704916 acc 0.114 wl 0.00024271396 train True
step 9400 loss 0.122462116 baseline: 0.12232009068704916 acc 0.11 wl 0.00014590712 train True
step 9450 loss 0.1222781 baseline: 0.12232009068704916 acc 0.152 wl 0.00016308538 train True
step 9500 loss 0.12231947 baseline: 0.12232009068704916 acc 0.134 wl 0.0001488412 train True
step 9550 loss 0.12236443 baseline: 0.12232009068704916 acc 0.136 wl 0.00011696262 train True
step 9600 loss 0.12242153 baseline: 0.12232009068704916 acc 0.128 wl 0.00015559392 train True
step 9650 loss 0.122283824 baseline: 0.12232009068704916 acc 0.128 wl 0.00017260399 train True
step 9700 loss 0.12229748 baseline: 0.12232009068704916 acc 0.13 wl 0.00012991468 train True
step 9750 loss 0.12243798 baseline: 0.12232009068704916 acc 0.118 wl 0.00021036461 train True
step 9800 loss 0.12231936 baseline: 0.12232009068704916 acc 0.128 wl 0.00018530674 train True
step 9850 loss 0.12240511 baseline: 0.12232009068704916 acc 0.108 wl 0.00019868027 train True
step 9900 loss 0.12228567 baseline: 0.12232009068704916 acc 0.126 wl 0.000115757735 train True
step 9950 loss 0.12239087 baseline: 0.12232009068704916 acc 0.122 wl 0.000110208624 train True
step 10000 loss 0.1222716 baseline: 0.12232009068704916 acc 0.14 wl 0.00011603486 train True
step 10050 loss 0.12222511 baseline: 0.12232009068704916 acc 0.124 wl 0.00010802965 train True
step 10100 loss 0.12238958 baseline: 0.12232009068704916 acc 0.118 wl 0.00011911774 train True
step 10150 loss 0.12241819 baseline: 0.12232009068704916 acc 0.108 wl 0.00010208722 train True
step 10200 loss 0.122362204 baseline: 0.12232009068704916 acc 0.128 wl 0.000111981484 train True
step 10250 loss 0.12243108 baseline: 0.12232009068704916 acc 0.114 wl 0.000114341696 train True
step 10300 loss 0.122184716 baseline: 0.12232009068704916 acc 0.152 wl 0.00012175328 train True
step 10350 loss 0.12216774 baseline: 0.12232009068704916 acc 0.134 wl 0.00015694683 train True
step 10400 loss 0.12244044 baseline: 0.12232009068704916 acc 0.118 wl 0.00014843297 train True
step 10450 loss 0.12243999 baseline: 0.12232009068704916 acc 0.1 wl 0.00012643689 train True
step 10500 loss 0.122322164 baseline: 0.12232009068704916 acc 0.128 wl 0.00013864509 train True
step 10550 loss 0.12231922 baseline: 0.12232009068704916 acc 0.124 wl 0.000117325966 train True
step 10600 loss 0.1223805 baseline: 0.12232009068704916 acc 0.108 wl 0.00011985387 train True
step 10650 loss 0.122457676 baseline: 0.12232009068704916 acc 0.13 wl 0.00011276468 train True
step 10700 loss 0.1222781 baseline: 0.12232009068704916 acc 0.12 wl 0.00011718295 train True
step 10750 loss 0.12238021 baseline: 0.12232009068704916 acc 0.122 wl 0.00010413351 train True
step 10800 loss 0.12228662 baseline: 0.12232009068704916 acc 0.128 wl 0.00010940715 train True
step 10850 loss 0.12229752 baseline: 0.12232009068704916 acc 0.116 wl 9.411291e-05 train True
step 10900 loss 0.12243259 baseline: 0.12232009068704916 acc 0.116 wl 0.00011262394 train True
step 10950 loss 0.1221606 baseline: 0.12232009068704916 acc 0.146 wl 0.00012680714 train True
step 11000 loss 0.12238778 baseline: 0.12232009068704916 acc 0.112 wl 0.00013973733 train True
step 11050 loss 0.12228406 baseline: 0.12232009068704916 acc 0.148 wl 0.00019711861 train True
step 11100 loss 0.12226162 baseline: 0.12232009068704916 acc 0.146 wl 0.00018195741 train True
step 11150 loss 0.12234625 baseline: 0.12232009068704916 acc 0.116 wl 0.00019721812 train True
step 11200 loss 0.12227237 baseline: 0.12232009068704916 acc 0.126 wl 0.00011087685 train True
step 11250 loss 0.1224296 baseline: 0.12232009068704916 acc 0.118 wl 0.000115177994 train True
step 11300 loss 0.1223832 baseline: 0.12232009068704916 acc 0.124 wl 0.0001232388 train True
step 11350 loss 0.12234118 baseline: 0.12232009068704916 acc 0.116 wl 0.00013638673 train True
step 11400 loss 0.12243067 baseline: 0.12232009068704916 acc 0.112 wl 0.00013302005 train True
step 11450 loss 0.122339725 baseline: 0.12232009068704916 acc 0.134 wl 0.00013598058 train True
step 11500 loss 0.12239997 baseline: 0.12232009068704916 acc 0.132 wl 0.00015716397 train True
step 11550 loss 0.122429214 baseline: 0.12232009068704916 acc 0.114 wl 0.00013843185 train True
step 11600 loss 0.12229663 baseline: 0.12232009068704916 acc 0.134 wl 0.00014326669 train True
step 11650 loss 0.12222593 baseline: 0.12232009068704916 acc 0.136 wl 0.00012243558 train True
step 11700 loss 0.12184721 baseline: 0.12232009068704916 acc 0.16 wl 0.000119541684 train True
step 11750 loss 0.121829115 baseline: 0.12232009068704916 acc 0.126 wl 0.0001309005 train True
step 11800 loss 0.12128672 baseline: 0.12232009068704916 acc 0.168 wl 0.00015138197 train True
step 11850 loss 0.120989226 baseline: 0.12232009068704916 acc 0.178 wl 0.00017571298 train True
step 11900 loss 0.12060118 baseline: 0.12232009068704916 acc 0.2 wl 0.00013614363 train True
step 11950 loss 0.12048371 baseline: 0.12232009068704916 acc 0.162 wl 0.00014894569 train True
training done... testing ...
step 0 loss 0.12023523 baseline: 0.12232009068704916 acc 0.162 wl 0.00025412775 train False
step 50 loss 0.120347306 baseline: 0.12232009068704916 acc 0.188 wl 0.00025412775 train False
step 100 loss 0.12109306 baseline: 0.12232009068704916 acc 0.17 wl 0.00025412775 train False
step 150 loss 0.119949676 baseline: 0.12232009068704916 acc 0.192 wl 0.00025412775 train False
test_el_total 100000.0 test_acc_sum 16843.0
test loss mean 0.12055672 test acc mean 0.16843 pt 26805
time: 2019-12-24 23:01:35.592200 experiment took 51275.23210096359 [s]
